{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_2_testing_hate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNxvPf9QWB1J6WL/QJG7lqz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Usama-zia/Project-2/blob/main/project_2_testing_hate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXxZBCpyj5CL"
      },
      "source": [
        "#Evaluation on pretrained transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7yL1wSbB8wM"
      },
      "source": [
        "#Installing Libraries and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCHIjlxlipGp",
        "outputId": "5377890f-13ea-40b6-f5fd-33a7e4f5f154"
      },
      "source": [
        "#Installing libraries and Dependencies\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install emoji\n",
        "!pip install demoji\n",
        "import emoji\n",
        "import demoji\n",
        "from transformers import RobertaTokenizer,TFRobertaForSequenceClassification\n",
        "from transformers import TFTrainer, TFTrainingArguments\n",
        "from transformers import XLMTokenizer, TFXLMForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import math\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud\n",
        "from textwrap import wrap\n",
        "from textblob import TextBlob\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 14.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 19.8MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 22.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 24.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 23.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 17.2MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 12.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 12.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 13.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 14.8MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 14.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 14.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 14.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 14.8MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 14.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 14.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 14.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 17.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 17.5MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n",
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.4 demoji-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y7QyliHwvDv"
      },
      "source": [
        "#Cloning the Dataset from Github Repository(https://github.com/Usama-zia/Project-2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fae8EReckGsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab482b7d-1aea-4ef0-93d9-a5f2c94e06b5"
      },
      "source": [
        "# remove dataset directory if already exists\n",
        "!rm -rf project-2\n",
        "\n",
        "# fetch dataset\n",
        "!git clone https://github.com/Usama-zia/Project-2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Project-2'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 45 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gznLtk4yZDp"
      },
      "source": [
        "#Function for preprocessing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbzqZvQpyiBo"
      },
      "source": [
        "\n",
        "def preprocessing_data(data):\n",
        "  print('============Before Preprocessing=============')\n",
        "\n",
        "  for index,text in enumerate(data[0][5:10]):\n",
        "    print('tweet %d:\\n'%(index+1),text)\n",
        "  #removing punctuations\n",
        "  data[0]=data[0].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
        "  #removing extra spaces\n",
        "  data[0]=data[0].apply(lambda x: re.sub(' +',' ',x))\n",
        "  #removing @user\n",
        "  data[0] = data[0].str.strip('user')\n",
        "\n",
        "  #after Preprocessing\n",
        "  print('============After Preprocessing=============')\n",
        "  for index,text in enumerate(data[0][0:5]):\n",
        "    print('tweet %d:\\n'%(index+1),text)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOCL3eXhxLL5"
      },
      "source": [
        "#Some Constant Used for Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bt18UDBk7SS"
      },
      "source": [
        "#folders = [emotion, hate ,sentiment]\n",
        "emotion = '/content/Project-2/datasets/emotion'\n",
        "hate = '/content/Project-2/datasets/hate'\n",
        "sentiment = '/content/Project-2/datasets/sentiment'\n",
        "train_text = 'train_text.txt'\n",
        "train_label = 'train_labels.txt'\n",
        "test_text = 'test_text.txt'\n",
        "test_label = 'test_labels.txt'\n",
        "val_text = 'val_text.txt'\n",
        "val_label = 'val_labels.txt'\n",
        "maps = 'mapping.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3dA1nPVxxJS"
      },
      "source": [
        "#Function for Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTKGcZzVrXpA"
      },
      "source": [
        "data_set=emotion #variable is declared outsids function so os.path.join does not through an error on compile\n",
        "def load_data(data_set):\n",
        "  #for loading train text and labels\n",
        "  with open(os.path.join(data_set, train_text)) as f:\n",
        "    train_texts = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "    with open(os.path.join(data_set,train_label)) as f:\n",
        "      train_labels = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "\n",
        "  with open(os.path.join(data_set, val_text)) as f:\n",
        "    val_texts = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "    with open(os.path.join(data_set, val_label)) as f:\n",
        "      val_labels = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "\n",
        "  with open(os.path.join(data_set, test_text)) as f:\n",
        "    test_texts = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "    with open(os.path.join(data_set, test_label)) as f:\n",
        "      test_labels = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "  return train_texts, train_labels,val_texts,val_labels,test_texts,test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP6xwMcBn4HC"
      },
      "source": [
        "#Loading and preprocessing hate dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQe3lkS2n9kj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17061e66-64f0-416a-d58a-2351233f9daa"
      },
      "source": [
        "train_texts,train_labels,val_texts,val_labels,test_texts,test_labels=load_data(hate)\n",
        "train_data = preprocessing_data(train_texts)\n",
        "val_data = preprocessing_data(val_texts)\n",
        "test_data = preprocessing_data(test_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============Before Preprocessing=============\n",
            "tweet 1:\n",
            " Me flirting- So tell me about your father... \n",
            "tweet 2:\n",
            " The Philippine Catholic bishops' work for migrant workers should focus on families who are \"paying the great... \n",
            "tweet 3:\n",
            " I AM NOT GOING AFTER YOUR EX BF YOU LIEING SACK OF SHIT ! I'm done with you dude that's why I dumped your ass cause your a lieing 😂😡 bitch \n",
            "tweet 4:\n",
            " When cuffin season is finally over \n",
            "tweet 5:\n",
            " Send home migrants not in need of protection, Peter Dutton tells UN, HEY DUTTON HOW ABOUT THE ONES THAT HAVE STAYED AND NOT LEFT THE COUNTRY WHEN THEY SHOULD OVERSTAYERS ? WHY DONT YOU GO AND ROUND ALL THEM UP ?  \n",
            "============After Preprocessing=============\n",
            "tweet 1:\n",
            "  nice new signage Are you not concerned by Beatlemania style hysterical crowds crongregating on you… \n",
            "tweet 2:\n",
            " A woman who you fucked multiple times saying yo dick small is a compliment you know u hit that spot 😎 \n",
            "tweet 3:\n",
            "  user real talk do you have eyes or were they gouged out by a rapefugee \n",
            "tweet 4:\n",
            " your girlfriend lookin at me like a groupie in this bitch \n",
            "tweet 5:\n",
            " Hysterical woman like user \n",
            "============Before Preprocessing=============\n",
            "tweet 1:\n",
            " Salvini reiterates hard line on migrants during Libya visit \n",
            "tweet 2:\n",
            " @user Shut the fuck up you pussy grabbing pervert.How's the divorce?It's Mueller Time! \n",
            "tweet 3:\n",
            " LaPlace,La. Illegal Alien arrested on a 2nd degree attempted murder charge for Machete attack. @user #BuildTheWallNow #NoAmnesty #NoDACA#DeportThemAll #KeepAmericaSafe \n",
            "tweet 4:\n",
            " Global Compact on Migration moves forward #WithRefugees #GlobalCompact @user @user @user @user @user \n",
            "tweet 5:\n",
            " CBI calls for favourable treatment of #EU migrants after #Brexit \n",
            "============After Preprocessing=============\n",
            "tweet 1:\n",
            "  user If book Claire wanted to stay in the kitchen cooking for her man she wouldve stayed in Inverness while Frank did his research and the Outlander series wouldve ended up being less than two chapters long Perhaps thats what you want but I prefer the story that we did get \n",
            "tweet 2:\n",
            " After arriving in the EU refugees make protest and want money Welcome or not \n",
            "tweet 3:\n",
            " 😳👇 \n",
            "tweet 4:\n",
            "  Worst thing is if they are that stupid to think he all by himself can roll back Roe v Wade then they really need to educate themselves on how SCOTUS works They are probably being paid like the other hysterical women \n",
            "tweet 5:\n",
            "  Says the HYSTERICAL woman It is woman right \n",
            "============Before Preprocessing=============\n",
            "tweet 1:\n",
            " @user @user @user Always #NoDACA.I AM BORN IN #USA AND #USA FIRST. \n",
            "tweet 2:\n",
            " @user friends ? that's my BLOOD brother bitch .. he in yo city !! i'm hours away . ill be through there tomo… \n",
            "tweet 3:\n",
            " @user @user Muslims attacked US on 9/11, 3000 killed. Subsequently we allowed more Muslims in our country?!?Does this make any sense to you?You can thank Obama, his liberals minions, RINO's & political correctness. #MuslimBan #BanIslam #TravelBan #DrainTheDeepState #DeportThemAll \n",
            "tweet 4:\n",
            " @user @user Like he ever kept out any threats. He's lying as usual. #BuildThatWall \n",
            "tweet 5:\n",
            " #germany deserves to called #cuck/land they allow in #isis fighters as #Refugees even if they raped underaged girls. this #yazidi girl is scared of this and she is forced to see the face of her rapist.#refugeesnotwelcome should apply to these scum \n",
            "============After Preprocessing=============\n",
            "tweet 1:\n",
            "  you are correct that Reid certainly is a weasel Sadly weve got our own weasels user Sen McConnell user The corrupt Mueller investigation wbe STOPPED if those 3 did their jobsMAGA KAG POTUS Trump NEWS VoteRed NoDACA USA \n",
            "tweet 2:\n",
            " Whoever just unfollowed me you a bitch \n",
            "tweet 3:\n",
            "  user Those People Invaded Us They DO NOT BELING HERE HAVE NO RIGHTS Its AmericaFIRST Open Your House To Them If Your That IGNORANT Yes Im A Christian Too NODACA \n",
            "tweet 4:\n",
            " top JUDGING bitches by there cover jus cuz she bad dont mean shes a catch shawdy could be a whore 👀 das opposite of a keeper \n",
            "tweet 5:\n",
            " how about i knock heads off and send them gift wrapped to your moms house you dumb raggedy bird bitch ass hoes \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM2EXDqnqORJ"
      },
      "source": [
        "#Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsODPigwrX5M"
      },
      "source": [
        "def compute_metrics(pred,labels):\n",
        "    labels = labels\n",
        "    preds = pred\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWcGxXpwqUgP"
      },
      "source": [
        "MODEL = f\"cardiffnlp/twitter-roberta-base-hate\"\n",
        "pred=[]\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "model.eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "for text in test_data[0]:\n",
        "  encoded_input = tokenizer(text, return_tensors='pt')\n",
        "  pred.append(model(**encoded_input)[0].argmax().item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwYBPUEqgXv"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oa4ZVdQBFNL"
      },
      "source": [
        "labels=[]\n",
        "for l in test_labels[0]:\n",
        "  labels.append(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx5rhWC6_qYm"
      },
      "source": [
        "result=compute_metrics(pred,labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUKBlEPECoIq"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}