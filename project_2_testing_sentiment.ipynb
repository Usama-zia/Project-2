{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_2_testing_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0hac9VBIB/Sv+wfdem09C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Usama-zia/Project-2/blob/main/project_2_testing_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXxZBCpyj5CL"
      },
      "source": [
        "#Evaluation on pretrained transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7yL1wSbB8wM"
      },
      "source": [
        "#Installing Libraries and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCHIjlxlipGp",
        "outputId": "02a7971c-59f1-4ea4-90b7-682706917bcc"
      },
      "source": [
        "#Installing libraries and Dependencies\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install emoji\n",
        "!pip install demoji\n",
        "import emoji\n",
        "import demoji\n",
        "from transformers import RobertaTokenizer,TFRobertaForSequenceClassification\n",
        "from transformers import TFTrainer, TFTrainingArguments\n",
        "from transformers import XLMTokenizer, TFXLMForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import math\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from wordcloud import WordCloud\n",
        "from textwrap import wrap\n",
        "from textblob import TextBlob\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import csv\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 26.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 25.7MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 29.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 27.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 30.2MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 22.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 23.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 20.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 20.8MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 20.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 20.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 20.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 20.8MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 20.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 20.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 20.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 20.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 18.4MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n",
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/88/6a/34379abe01c9c36fe9fddc4181dd935332e7d0159ec3fae76f712e49bcea/demoji-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.7/dist-packages (from demoji) (2.23.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.4 demoji-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y7QyliHwvDv"
      },
      "source": [
        "#Cloning the Dataset from Github Repository(https://github.com/Usama-zia/Project-2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fae8EReckGsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2fc741-03e5-4ce7-82d6-52baf3cb35bf"
      },
      "source": [
        "# remove dataset directory if already exists\n",
        "!rm -rf project-2\n",
        "\n",
        "# fetch dataset\n",
        "!git clone https://github.com/Usama-zia/Project-2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Project-2'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 45 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gznLtk4yZDp"
      },
      "source": [
        "#Function for preprocessing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbzqZvQpyiBo"
      },
      "source": [
        "\n",
        "def preprocessing_data(data):\n",
        "  print('============Before Preprocessing=============')\n",
        "\n",
        "  for index,text in enumerate(data[0][5:10]):\n",
        "    print('tweet %d:\\n'%(index+1),text)\n",
        "  #removing punctuations\n",
        "  data[0]=data[0].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n",
        "  #removing extra spaces\n",
        "  data[0]=data[0].apply(lambda x: re.sub(' +',' ',x))\n",
        "  #removing @user\n",
        "  data[0] = data[0].str.strip('user')\n",
        "\n",
        "  #after Preprocessing\n",
        "  print('============After Preprocessing=============')\n",
        "  for index,text in enumerate(data[0][0:5]):\n",
        "    print('tweet %d:\\n'%(index+1),text)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOCL3eXhxLL5"
      },
      "source": [
        "#Some Constant Used for Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bt18UDBk7SS"
      },
      "source": [
        "#folders = [emotion, hate ,sentiment]\n",
        "emotion = '/content/Project-2/datasets/emotion'\n",
        "hate = '/content/Project-2/datasets/hate'\n",
        "sentiment = '/content/Project-2/datasets/sentiment'\n",
        "train_text = 'train_text.txt'\n",
        "train_label = 'train_labels.txt'\n",
        "test_text = 'test_text.txt'\n",
        "test_label = 'test_labels.txt'\n",
        "val_text = 'val_text.txt'\n",
        "val_label = 'val_labels.txt'\n",
        "maps = 'mapping.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3dA1nPVxxJS"
      },
      "source": [
        "#Function for Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTKGcZzVrXpA"
      },
      "source": [
        "data_set=emotion #variable is declared outsids function so os.path.join does not through an error on compile\n",
        "def load_data(data_set):\n",
        "  #for loading train text and labels\n",
        "  with open(os.path.join(data_set, train_text)) as f:\n",
        "    train_texts = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "    with open(os.path.join(data_set,train_label)) as f:\n",
        "      train_labels = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "\n",
        "  with open(os.path.join(data_set, val_text)) as f:\n",
        "    val_texts = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "    with open(os.path.join(data_set, val_label)) as f:\n",
        "      val_labels = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "\n",
        "  with open(os.path.join(data_set, test_text)) as f:\n",
        "    test_texts = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "    with open(os.path.join(data_set, test_label)) as f:\n",
        "      test_labels = pd.read_csv(f, sep=\"\\n\", header=None)\n",
        "  return train_texts, train_labels,val_texts,val_labels,test_texts,test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP6xwMcBn4HC"
      },
      "source": [
        "#Loading and preprocessing sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQe3lkS2n9kj"
      },
      "source": [
        "train_texts,train_labels,val_texts,val_labels,test_texts,test_labels=load_data(sentiment)\n",
        "train_data = preprocessing_data(train_texts)\n",
        "val_data = preprocessing_data(val_texts)\n",
        "test_data = preprocessing_data(test_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM2EXDqnqORJ"
      },
      "source": [
        "#Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsODPigwrX5M"
      },
      "source": [
        "def compute_metrics(pred,labels):\n",
        "    labels = labels\n",
        "    preds = pred\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWcGxXpwqUgP"
      },
      "source": [
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "pred=[]\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "model.eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "for text in test_data[0]:\n",
        "  encoded_input = tokenizer(text, return_tensors='pt')\n",
        "  pred.append(model(**encoded_input)[0].argmax().item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwYBPUEqgXv"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oa4ZVdQBFNL"
      },
      "source": [
        "labels=[]\n",
        "for l in test_labels_emotion[0]:\n",
        "  labels.append(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx5rhWC6_qYm"
      },
      "source": [
        "result=compute_metrics(pred,labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUKBlEPECoIq"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}